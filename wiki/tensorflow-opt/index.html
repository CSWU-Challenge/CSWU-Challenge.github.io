<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Southeast University HPC platform."><meta name=author content=CSWU-Challenge><link href=https://CSWU-Challenge.github.io/wiki/tensorflow-opt/ rel=canonical><link rel=icon href=../../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-8.2.5"><title>TensorFlow模型优化思路 - 东南大学超算平台</title><link rel=stylesheet href=../../assets/stylesheets/main.2d9f7617.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.e6a45f82.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-7Y7KMG2XPN"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7Y7KMG2XPN');
</script><link rel=stylesheet href=../../overrides/assets/stylesheets/main.d9227bb8.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=teal data-md-color-accent=teal> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#tensorflow class=md-skip> 跳转至 </a> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../.. title=东南大学超算平台 class="md-header__button md-logo" aria-label=东南大学超算平台 data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> 东南大学超算平台 </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> TensorFlow模型优化思路 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=teal data-md-color-accent=teal aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=black aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31z"/></svg> </label> </form> <div class=md-header__source> <a href=https://github.com/CSWU-Challenge/CSWU-Challenge.github.io title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> CSWU-Challeng </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../ASC-introduction/ class="md-tabs__link md-tabs__link--active"> Wiki </a> </li> <li class=md-tabs__item> <a href=../../page/high-light/ class=md-tabs__link> High Light </a> </li> <li class=md-tabs__item> <a href=../../page/contributor/ class=md-tabs__link> Contirbutor </a> </li> <li class=md-tabs__item> <a href=../../page/to-contributors/ class=md-tabs__link> How-To </a> </li> <li class=md-tabs__item> <a href=../../page/Download/ class=md-tabs__link> Download </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title=东南大学超算平台 class="md-nav__button md-logo" aria-label=东南大学超算平台 data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> 东南大学超算平台 </label> <div class=md-nav__source> <a href=https://github.com/CSWU-Challenge/CSWU-Challenge.github.io title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> CSWU-Challeng </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2 type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2> Wiki <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Wiki data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Wiki </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ASC-introduction/ class=md-nav__link> 超算竞赛快速入门 </a> </li> <li class=md-nav__item> <a href=../Linux-base/ class=md-nav__link> Linux基础教程 </a> </li> <li class=md-nav__item> <a href=../conda%26pip_base/ class=md-nav__link> conda与pip基础 </a> </li> <li class=md-nav__item> <a href=../Proposal-writing/ class=md-nav__link> 如何写Proposal </a> </li> <li class=md-nav__item> <a href=../vimtutor/ class=md-nav__link> VIM指令入门 </a> </li> <li class=md-nav__item> <a href=../pytorch-opt/ class=md-nav__link> PyTorch模型优化思路 </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> TensorFlow模型优化思路 <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> TensorFlow模型优化思路 </a> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> 前言 </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> 输入流水线优化 </a> <nav class=md-nav aria-label=输入流水线优化> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cpu class=md-nav__link> 在CPU上预处理 </a> </li> <li class=md-nav__item> <a href=#tfdata-api class=md-nav__link> 使用tf.data API </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> 融合解码与裁剪 </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> 使用大文件 </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> 数据格式 </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> 常见的融合操作 </a> <nav class=md-nav aria-label=常见的融合操作> <ul class=md-nav__list> <li class=md-nav__item> <a href=#fused-batch-norm class=md-nav__link> Fused batch norm </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> 从源码编译安装 </a> </li> <li class=md-nav__item> <a href=#gpu class=md-nav__link> 针对GPU进行优化 </a> </li> <li class=md-nav__item> <a href=#cpu_1 class=md-nav__link> 针对CPU进行优化 </a> </li> <li class=md-nav__item> <a href=#intel-mkl-dnntensorflow class=md-nav__link> 使用Intel® MKL DNN的Tensorflow </a> <nav class=md-nav aria-label="使用Intel® MKL DNN的Tensorflow"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#mkl class=md-nav__link> 调整MKL以获得最佳性能 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../latex-manual/ class=md-nav__link> Latex环境配置及编译优化 </a> </li> <li class=md-nav__item> <a href=../parallel_coding/ class=md-nav__link> c++并行编程入门 </a> </li> <li class=md-nav__item> <a href=../how-gui-work/ class=md-nav__link> 从内存到屏幕：GUI 显示手段的进化史 </a> </li> <li class=md-nav__item> <a href=../cuda-project/ class=md-nav__link> cuda编程(待完善) </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../page/high-light/ class=md-nav__link> High Light </a> </li> <li class=md-nav__item> <a href=../../page/contributor/ class=md-nav__link> Contirbutor </a> </li> <li class=md-nav__item> <a href=../../page/to-contributors/ class=md-nav__link> How-To </a> </li> <li class=md-nav__item> <a href=../../page/Download/ class=md-nav__link> Download </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> 前言 </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> 输入流水线优化 </a> <nav class=md-nav aria-label=输入流水线优化> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cpu class=md-nav__link> 在CPU上预处理 </a> </li> <li class=md-nav__item> <a href=#tfdata-api class=md-nav__link> 使用tf.data API </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> 融合解码与裁剪 </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> 使用大文件 </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_5 class=md-nav__link> 数据格式 </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> 常见的融合操作 </a> <nav class=md-nav aria-label=常见的融合操作> <ul class=md-nav__list> <li class=md-nav__item> <a href=#fused-batch-norm class=md-nav__link> Fused batch norm </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_7 class=md-nav__link> 从源码编译安装 </a> </li> <li class=md-nav__item> <a href=#gpu class=md-nav__link> 针对GPU进行优化 </a> </li> <li class=md-nav__item> <a href=#cpu_1 class=md-nav__link> 针对CPU进行优化 </a> </li> <li class=md-nav__item> <a href=#intel-mkl-dnntensorflow class=md-nav__link> 使用Intel® MKL DNN的Tensorflow </a> <nav class=md-nav aria-label="使用Intel® MKL DNN的Tensorflow"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#mkl class=md-nav__link> 调整MKL以获得最佳性能 </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=tensorflow>Tensorflow模型优化思路<a class=headerlink href=#tensorflow title="Permanent link">&para;</a></h1> <h2 id=_1>前言<a class=headerlink href=#_1 title="Permanent link">&para;</a></h2> <p>&emsp;&emsp;近年来AI模型成为ASC竞赛选题的香饽饽，如2022年的三四两题分别是基于pytorch的大型语言模型，和基于tensorflow的分子动力学模型。AI模型的优化可以从许多方面入手，本文参考w3cub的文档<a href="https://docs.w3cub.com/tensorflow~guide/performance/performance_guide.html#:~:text=Inference%20InceptionV3%20%20%20%20Optimization%20%20,%20%202.8%20%28361ms%29%20%20%200%20?msclkid=00c39d61a53711eca0957ae5f013dd6a">Tensorflow Guide</a>，对其翻译与总结，介绍Tensorflow框架的通用优化思路。</p> <p>&emsp;&emsp;<strong>注意：本文所讲方法仅供参考，帮助大家打开思路。你可能会发现这些方法并不会有太大效果，甚至没有效果。如果想有所突破，必须靠自己深度阅读代码，对症下药。</strong></p> <h2 id=_2>输入流水线优化<a class=headerlink href=#_2 title="Permanent link">&para;</a></h2> <p>&emsp;&emsp;典型的模型从磁盘检索数据并在通过网络发送数据之前对其进行预处理。例如，处理JPEG图像的模型将遵循以下流程：从磁盘加载图像，将JPEG解码为张量，裁剪和填充，可能会翻转和扭曲，然后批量处理。这个流程被称为输入流水线。随着GPU和其他硬件加速器变得更快，数据预处理可能成为瓶颈。</p> <p>&emsp;&emsp;确定流水线是否是瓶颈可能很复杂。最简单的方法之一是在流水线之后将模型简化为单个操作（平凡模型）并每秒测量示例。如果整个模型和平凡模型的每秒示例差异最小，则流水线可能是瓶颈。以下是一些确定问题的其他方法： - 通过运行<code>nvidia-smi</code>检查GPU是否未充分利用。如果GPU利用率未达到80-100％，则流水线可能是瓶颈。 - 生成一个时间表并查找大块空白（等待）。在XLA JIT教程中，存在一个生成时间线的例子。 - 检查CPU使用情况。很有可能存在着可优化的流水线，并且缺乏CPU周期来处理流水线。 - 估计所需的吞吐量并验证使用的磁盘是否具有该吞吐量。一些云服务器的网络连接磁盘的启动速度低于50 MB /秒，比机械磁盘（150 MB /秒），SATA SSD（500 MB /秒）和PCIe SSD（2,000 MB /秒）慢。</p> <h3 id=cpu>在CPU上预处理<a class=headerlink href=#cpu title="Permanent link">&para;</a></h3> <p>在CPU上放置输入管道操作可显着提高性能。利用输入管道的CPU，GPU可以将精力集中在训练上。为确保预处理在CPU上，请按如下所示包装预处理操作：</p> <p><div class=highlight><pre><span></span><code><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>with tf.device(&#39;/cpu:0&#39;):
<a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>  # function to get and process images or data.
<a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>  distorted_inputs = load_and_distort_images()
</code></pre></div> 如果使用<code>tf.estimator.Estimator</code>，则输入功能自动放置在CPU上。</p> <h3 id=tfdata-api>使用tf.data API<a class=headerlink href=#tfdata-api title="Permanent link">&para;</a></h3> <p>&emsp;&emsp;tf.data API将替代<code>queue_runner</code>，作为推荐的构建流水线的API。 tf.data API利用了C++的多线程，并且比基于python的且受限于python多线程性能的<code>queue_runner</code>有更低的开销。</p> <p>&emsp;&emsp;虽然使用<code>feed_dict</code>来feed data有很高的灵活性，但一般而言<code>feed_dict</code>不适用于大规模的模型。如果只用一个GPU，那么tf.data API和<code>feed_dict</code>之间的性能差距可以被忽略。我们建议避免使用<code>feed_dict</code>，除非你的模型规模很小。尤其避免在输入数据量很大的情况下使用<code>feed_dict</code>。 <div class=highlight><pre><span></span><code><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a># feed_dict often results in suboptimal performance when using large inputs.
<a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
</code></pre></div></p> <h3 id=_3>融合解码与裁剪<a class=headerlink href=#_3 title="Permanent link">&para;</a></h3> <p>&emsp;&emsp;如果输入数据是需要裁剪的jpeg图像，使用融合的 <a href=https://www.tensorflow.org/api_docs/python/tf/image/decode_and_crop_jpeg><code>tf.image.decode_and_crop_jpeg</code></a>来加速预处理。<a href=https://www.tensorflow.org/api_docs/python/tf/image/decode_and_crop_jpeg><code>tf.image.decode_and_crop_jpeg</code></a>只会解码剪裁后剩余的图片。如果剪裁窗口远远小于整张图片，这将会有很大程度的加速效果。 使用样例： <div class=highlight><pre><span></span><code><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a>def _image_preprocess_fn(image_buffer):
<a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>    # image_buffer 1-D string Tensor representing the raw JPEG image buffer.
<a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>
<a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>    # Extract image shape from raw JPEG image buffer.
<a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a>    image_shape = tf.image.extract_jpeg_shape(image_buffer)
<a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a>
<a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a>    # Get a crop window with distorted bounding box.
<a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a>    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(
<a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a>      image_shape, ...)
<a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a>    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box
<a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a>
<a id=__codelineno-2-12 name=__codelineno-2-12 href=#__codelineno-2-12></a>    # Decode and crop image.
<a id=__codelineno-2-13 name=__codelineno-2-13 href=#__codelineno-2-13></a>    offset_y, offset_x, _ = tf.unstack(bbox_begin)
<a id=__codelineno-2-14 name=__codelineno-2-14 href=#__codelineno-2-14></a>    target_height, target_width, _ = tf.unstack(bbox_size)
<a id=__codelineno-2-15 name=__codelineno-2-15 href=#__codelineno-2-15></a>    crop_window = tf.stack([offset_y, offset_x, target_height, target_width])
<a id=__codelineno-2-16 name=__codelineno-2-16 href=#__codelineno-2-16></a>    cropped_image = tf.image.decode_and_crop_jpeg(image, crop_window)
</code></pre></div></p> <h3 id=_4>使用大文件<a class=headerlink href=#_4 title="Permanent link">&para;</a></h3> <p>&emsp;&emsp;从小文件中读取大量数据非常影响I/O性能。一种增大I/O吞吐量的方法是将输入数据预处理到更大(~100MB)的<code>TRFRecord</code>文件中。对于更小的数据集(200MB-1GB)，最好的方法是将整个数据集加载到内存。这个<a href=https://github.com/tensorflow/models/tree/master/research/slim#downloading-and-converting-to-tfrecord-format>文档</a>介绍了<code>TFRecords</code>的相关内容。</p> <h2 id=_5>数据格式<a class=headerlink href=#_5 title="Permanent link">&para;</a></h2> <p>&emsp;&emsp;数据格式指传递给优化器的张量的结构。下面的讨论只针对表示图像的4D张量。在Tensorflow中，4D张量经常被以下字母指代： - N 指一个batch里的图片数量 - H 指在垂直维度里的像素数量 - W 指在水平维度里的像素数量 - C 指channel数。比如说灰度图的channel为1，RGB图的channel为3</p> <p>在tensorflow中有两种传统的命名来表示两个最常见的数据格式 - <code>NCHW</code> or <code>channels_first</code> - <code>NHWC</code> or <code>channels_last</code></p> <p><code>NHWC</code>是Tensorflow的默认格式，然而如果使用cuDNN在NVIDIA GPU上训练，那么<code>NCHW</code>是最优格式。</p> <p>&emsp;&emsp;最好的行为是用这两种格式同时构建模型。这简化了在GPU上的训练以及在CPU上的推理。如果tensorflow使用Intel MKL的优化来编译，那么许多操作，尤其是那些与基于CNN的模型有关的操作，将会被优化并且支持<code>NCHW</code>。如果不适用MKL，有些操作在使用<code>NCHW</code>时将不被CPU支持。</p> <h2 id=_6>常见的融合操作<a class=headerlink href=#_6 title="Permanent link">&para;</a></h2> <p>&emsp;&emsp;融合操作是指为了取得更好的性能而把多个操作组合进一个单独的计算核中。在Tensorflow中有许多融合操作，并且<a href=https://docs.w3cub.com/tensorflow~guide/performance/xla/index>XLA</a>也会创建融合操作，如果有可能自动提高性能的话。下面这个融合操作可以极大的提升性能并且可能会被忽略。</p> <h4 id=fused-batch-norm>Fused batch norm<a class=headerlink href=#fused-batch-norm title="Permanent link">&para;</a></h4> <p>&emsp;&emsp;Fused batch norm将多个batch normalization的操作组合为一个计算核。Batch norm对于一些需要大量操作时间的模型而言是一个昂贵的过程。使用fused batch norm可以有12%~30%的提速。</p> <p>&emsp;&emsp;有两个常用的batch norm，他们都支持融合。 <div class=highlight><pre><span></span><code><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a>bn = tf.layers.batch_normalization(
<a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a>    input_layer, fused=True, data_format=&#39;NCHW&#39;)
</code></pre></div> <div class=highlight><pre><span></span><code><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a>bn = tf.contrib.layers.batch_norm(input_layer, fused=True, data_format=&#39;NCHW&#39;)
</code></pre></div></p> <h2 id=_7>从源码编译安装<a class=headerlink href=#_7 title="Permanent link">&para;</a></h2> <p>&emsp;&emsp;Tensorflow默认的二进制安装包为了使每个人都能安装tensorflow，适配了各种各样的硬件配置。如果使用CPU来训练或者推理，建议使用当前CPU可用的所有优化选项来编译Tensorflow。</p> <p>&emsp;&emsp;为了安装最优版本的tensorflow，请从源码编译安装。如果需要在一个与tensorflow预设配置不同的平台上编译tensorflow，那么使用你平台上最高的优化选项来交叉编译(cross-compile)。下面的命令就是一个使用<code>bazel</code>在一个特定的平台上编译的例子。 <div class=highlight><pre><span></span><code><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a># This command optimizes for Intel’s Broadwell processor
<a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>bazel build -c opt --copt=-march=&quot;broadwell&quot; --config=cuda //tensorflow/tools/pip_package:build_pip_package
</code></pre></div></p> <h2 id=gpu>针对GPU进行优化<a class=headerlink href=#gpu title="Permanent link">&para;</a></h2> <p>&emsp;&emsp;本节包含一般最佳实践中未涉及的GPU特定提示。在多GPU上获得最佳性能是一项挑战。常用的方法是使用数据并行。通过使用数据并行性进行扩展包括制作模型的多个副本（称为“塔”），然后在每个GPU上放置一个塔。每个塔都运行在不同的小批量数据上，然后更新需要在每个塔之间共享的变量（也称为参数）。每个塔如何获得更新的变量以及梯度如何应用都会影响模型的性能，缩放和收敛性。本节的其余部分概述了多个GPU上的变量放置和模型高耸。</p> <p>处理变量更新的最佳方法取决于模型，硬件以及硬件的配置方式。一个例子是，两个系统可以使用NVIDIA Tesla P100构建，但可能使用PCIe和另一个NVLink。在这种情况下，每个系统的最佳解决方案可能会有所不同。有关真实的示例，请阅读基准页面，其中详细介绍了适用于各种平台的最佳设置。以下是对各种平台和配置进行基准测试所得到的总结：</p> <ul> <li>Tesla K80：如果GPU处于同一个PCI Express根联合体上，并且能够使用<code>NVIDIA GPUDirect Peer to Peer</code>，那么将这些变量平均放置在用于训练的GPU上是最好的方法。如果GPU不能使用GPUDirect，那么将变量放在CPU上是最好的选择。</li> <li>Titan X（Maxwell和Pascal），M40，P100等类似：对于像ResNet和InceptionV3这样的模型，将变量放置在CPU上是最佳设置，但对于像AlexNet和VGG等很多变量的模型，使用GPU <code>NCCL</code>更好。</li> </ul> <p>&emsp;&emsp;管理变量放置位置的常用方法是创建一个方法来确定每个Op的放置位置，并在调用时使用该方法代替特定的设备名称<code>with tf.device()</code>:。考虑在2个GPU上训练模型并将变量放置在CPU上的场景。将会有一个循环用于在两个GPU的每一个上创建和放置“塔”。自定义设备放置方法将被创建，用来监视Variable，VariableV2以及VarHandleOp和表明它们将被放置在CPU上。所有其他操作将被放置在目标GPU上。该图的构建过程如下：</p> <ul> <li>在第一个循环中，模型的“tower”将被创建在<code>gpu:0</code>。在放置操作期间，自定义设备放置方法将指示变量将被放置在<code>cpu:0</code>，所有其他操作放在<code>gpu:0</code>。</li> <li>在第二个循环中，<code>reuse</code>设置为<code>True</code>指示变量将被重用，然后在 <code>gpu:1</code>上创建塔。在放置与“塔”相关联的操作期间，<code>cpu:0</code>重复使用放置的变量，并创建、放置所有其他操作在<code>gpu:1</code>。</li> </ul> <p>&emsp;&emsp;最终的结果是所有的变量都放在CPU上，每个GPU都有与模型相关的所有计算操作的副本。</p> <p>&emsp;&emsp;下面的代码片段展示了两种不同的变量放置方式：一种是在CPU上放置变量; 另一个是在GPU上平均放置变量。 <div class=highlight><pre><span></span><code><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a>class GpuParamServerDeviceSetter(object):
<a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>  &quot;&quot;&quot;Used with tf.device() to place variables on the least loaded GPU.
<a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>
<a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>    A common use for this class is to pass a list of GPU devices, e.g. [&#39;gpu:0&#39;,
<a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>    &#39;gpu:1&#39;,&#39;gpu:2&#39;], as ps_devices.  When each variable is placed, it will be
<a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>    placed on the least loaded gpu. All other Ops, which will be the computation
<a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>    Ops, will be placed on the worker_device.
<a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a>  &quot;&quot;&quot;
<a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a>
<a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a>  def __init__(self, worker_device, ps_devices):
<a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a>    &quot;&quot;&quot;Initializer for GpuParamServerDeviceSetter.
<a id=__codelineno-6-12 name=__codelineno-6-12 href=#__codelineno-6-12></a>    Args:
<a id=__codelineno-6-13 name=__codelineno-6-13 href=#__codelineno-6-13></a>      worker_device: the device to use for computation Ops.
<a id=__codelineno-6-14 name=__codelineno-6-14 href=#__codelineno-6-14></a>      ps_devices: a list of devices to use for Variable Ops. Each variable is
<a id=__codelineno-6-15 name=__codelineno-6-15 href=#__codelineno-6-15></a>      assigned to the least loaded device.
<a id=__codelineno-6-16 name=__codelineno-6-16 href=#__codelineno-6-16></a>    &quot;&quot;&quot;
<a id=__codelineno-6-17 name=__codelineno-6-17 href=#__codelineno-6-17></a>    self.ps_devices = ps_devices
<a id=__codelineno-6-18 name=__codelineno-6-18 href=#__codelineno-6-18></a>    self.worker_device = worker_device
<a id=__codelineno-6-19 name=__codelineno-6-19 href=#__codelineno-6-19></a>    self.ps_sizes = [0] * len(self.ps_devices)
<a id=__codelineno-6-20 name=__codelineno-6-20 href=#__codelineno-6-20></a>
<a id=__codelineno-6-21 name=__codelineno-6-21 href=#__codelineno-6-21></a>  def __call__(self, op):
<a id=__codelineno-6-22 name=__codelineno-6-22 href=#__codelineno-6-22></a>    if op.device:
<a id=__codelineno-6-23 name=__codelineno-6-23 href=#__codelineno-6-23></a>      return op.device
<a id=__codelineno-6-24 name=__codelineno-6-24 href=#__codelineno-6-24></a>    if op.type not in [&#39;Variable&#39;, &#39;VariableV2&#39;, &#39;VarHandleOp&#39;]:
<a id=__codelineno-6-25 name=__codelineno-6-25 href=#__codelineno-6-25></a>      return self.worker_device
<a id=__codelineno-6-26 name=__codelineno-6-26 href=#__codelineno-6-26></a>
<a id=__codelineno-6-27 name=__codelineno-6-27 href=#__codelineno-6-27></a>    # Gets the least loaded ps_device
<a id=__codelineno-6-28 name=__codelineno-6-28 href=#__codelineno-6-28></a>    device_index, _ = min(enumerate(self.ps_sizes), key=operator.itemgetter(1))
<a id=__codelineno-6-29 name=__codelineno-6-29 href=#__codelineno-6-29></a>    device_name = self.ps_devices[device_index]
<a id=__codelineno-6-30 name=__codelineno-6-30 href=#__codelineno-6-30></a>    var_size = op.outputs[0].get_shape().num_elements()
<a id=__codelineno-6-31 name=__codelineno-6-31 href=#__codelineno-6-31></a>    self.ps_sizes[device_index] += var_size
<a id=__codelineno-6-32 name=__codelineno-6-32 href=#__codelineno-6-32></a>
<a id=__codelineno-6-33 name=__codelineno-6-33 href=#__codelineno-6-33></a>    return device_name
<a id=__codelineno-6-34 name=__codelineno-6-34 href=#__codelineno-6-34></a>
<a id=__codelineno-6-35 name=__codelineno-6-35 href=#__codelineno-6-35></a>def _create_device_setter(is_cpu_ps, worker, num_gpus):
<a id=__codelineno-6-36 name=__codelineno-6-36 href=#__codelineno-6-36></a>  &quot;&quot;&quot;Create device setter object.&quot;&quot;&quot;
<a id=__codelineno-6-37 name=__codelineno-6-37 href=#__codelineno-6-37></a>  if is_cpu_ps:
<a id=__codelineno-6-38 name=__codelineno-6-38 href=#__codelineno-6-38></a>    # tf.train.replica_device_setter supports placing variables on the CPU, all
<a id=__codelineno-6-39 name=__codelineno-6-39 href=#__codelineno-6-39></a>    # on one GPU, or on ps_servers defined in a cluster_spec.
<a id=__codelineno-6-40 name=__codelineno-6-40 href=#__codelineno-6-40></a>    return tf.train.replica_device_setter(
<a id=__codelineno-6-41 name=__codelineno-6-41 href=#__codelineno-6-41></a>        worker_device=worker, ps_device=&#39;/cpu:0&#39;, ps_tasks=1)
<a id=__codelineno-6-42 name=__codelineno-6-42 href=#__codelineno-6-42></a>  else:
<a id=__codelineno-6-43 name=__codelineno-6-43 href=#__codelineno-6-43></a>    gpus = [&#39;/gpu:%d&#39; % i for i in range(num_gpus)]
<a id=__codelineno-6-44 name=__codelineno-6-44 href=#__codelineno-6-44></a>    return ParamServerDeviceSetter(worker, gpus)
<a id=__codelineno-6-45 name=__codelineno-6-45 href=#__codelineno-6-45></a>
<a id=__codelineno-6-46 name=__codelineno-6-46 href=#__codelineno-6-46></a># The method below is a modified snippet from the full example.
<a id=__codelineno-6-47 name=__codelineno-6-47 href=#__codelineno-6-47></a>def _resnet_model_fn():
<a id=__codelineno-6-48 name=__codelineno-6-48 href=#__codelineno-6-48></a>    # When set to False, variables are placed on the least loaded GPU. If set
<a id=__codelineno-6-49 name=__codelineno-6-49 href=#__codelineno-6-49></a>    # to True, the variables will be placed on the CPU.
<a id=__codelineno-6-50 name=__codelineno-6-50 href=#__codelineno-6-50></a>    is_cpu_ps = False
<a id=__codelineno-6-51 name=__codelineno-6-51 href=#__codelineno-6-51></a>
<a id=__codelineno-6-52 name=__codelineno-6-52 href=#__codelineno-6-52></a>    # Loops over the number of GPUs and creates a copy (&quot;tower&quot;) of the model on
<a id=__codelineno-6-53 name=__codelineno-6-53 href=#__codelineno-6-53></a>    # each GPU.
<a id=__codelineno-6-54 name=__codelineno-6-54 href=#__codelineno-6-54></a>    for i in range(num_gpus):
<a id=__codelineno-6-55 name=__codelineno-6-55 href=#__codelineno-6-55></a>      worker = &#39;/gpu:%d&#39; % i
<a id=__codelineno-6-56 name=__codelineno-6-56 href=#__codelineno-6-56></a>      # Creates a device setter used to determine where Ops are to be placed.
<a id=__codelineno-6-57 name=__codelineno-6-57 href=#__codelineno-6-57></a>      device_setter = _create_device_setter(is_cpu_ps, worker, FLAGS.num_gpus)
<a id=__codelineno-6-58 name=__codelineno-6-58 href=#__codelineno-6-58></a>      # Creates variables on the first loop.  On subsequent loops reuse is set
<a id=__codelineno-6-59 name=__codelineno-6-59 href=#__codelineno-6-59></a>      # to True, which results in the &quot;towers&quot; sharing variables.
<a id=__codelineno-6-60 name=__codelineno-6-60 href=#__codelineno-6-60></a>      with tf.variable_scope(&#39;resnet&#39;, reuse=bool(i != 0)):
<a id=__codelineno-6-61 name=__codelineno-6-61 href=#__codelineno-6-61></a>        with tf.name_scope(&#39;tower_%d&#39; % i) as name_scope:
<a id=__codelineno-6-62 name=__codelineno-6-62 href=#__codelineno-6-62></a>          # tf.device calls the device_setter for each Op that is created.
<a id=__codelineno-6-63 name=__codelineno-6-63 href=#__codelineno-6-63></a>          # device_setter returns the device the Op is to be placed on.
<a id=__codelineno-6-64 name=__codelineno-6-64 href=#__codelineno-6-64></a>          with tf.device(device_setter):
<a id=__codelineno-6-65 name=__codelineno-6-65 href=#__codelineno-6-65></a>            # Creates the &quot;tower&quot;.
<a id=__codelineno-6-66 name=__codelineno-6-66 href=#__codelineno-6-66></a>            _tower_fn(is_training, weight_decay, tower_features[i],
<a id=__codelineno-6-67 name=__codelineno-6-67 href=#__codelineno-6-67></a>                      tower_labels[i], tower_losses, tower_gradvars,
<a id=__codelineno-6-68 name=__codelineno-6-68 href=#__codelineno-6-68></a>                      tower_preds, False
</code></pre></div></p> <h2 id=cpu_1>针对CPU进行优化<a class=headerlink href=#cpu_1 title="Permanent link">&para;</a></h2> <p>下面列出的两种配置用于通过调整线程池来优化CPU性能。 - <code>intra_op_parallelism_threads</code>：可以使用多个线程来并行执行的节点会将各个部分安排到该池中。 - <code>inter_op_parallelism_threads</code>：所有就绪节点都安排在此池中。</p> <p>&emsp;&emsp;这些配置通过tf.ConfigProto设置，并在config属性中传递给tf.Session，如下面的片段所示。对于这两个配置选项，如果不设置或设置为0，将默认为逻辑CPU核心的数量。测试表明，默认值对从一个4核的CPU到多个CPU的70多个组合逻辑核的系统都是有效的。一个常见的替代优化是将两个池中的线程数设置为等于物理核心数而不是逻辑核心数。 <div class=highlight><pre><span></span><code><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a>config = tf.ConfigProto()
<a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>config.intra_op_parallelism_threads = 44
<a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>config.inter_op_parallelism_threads = 44
<a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a>tf.session(config=config)
</code></pre></div></p> <h2 id=intel-mkl-dnntensorflow>使用Intel® MKL DNN的Tensorflow<a class=headerlink href=#intel-mkl-dnntensorflow title="Permanent link">&para;</a></h2> <h3 id=mkl>调整MKL以获得最佳性能<a class=headerlink href=#mkl title="Permanent link">&para;</a></h3> <p>&emsp;&emsp;本节详细介绍了不同的配置和环境变量，可用于调整MKL以获得最佳性能。在调整各种环境变量之前，请确保模型使用的是NCHW(channel_first)数据格式。MKL已经针对NCHW进行了优化，英特尔正在努力使使用NHWC时的性能接近平价。 - KMP_BLOCKTIME - 设置线程在完成一个并行区域的执行后，在睡眠前应该等待的时间，单位是毫秒。 - KMP_AFFINITY - 启用运行时库，将线程与物理处理单元绑定。 - KMP_SETTINGS - 在程序执行过程中启用（true）或禁用（false）打印OpenMP*运行时库的环境变量。 - OMP_NUM_THREADS - 指定要使用的线程数量。</p> <p>关于KMP详见<a href=https://software.intel.com/en-us/node/522775>intel</a>，关于OMP详见<a href=https://gcc.gnu.org/onlinedocs/libgomp/Environment-Variables.html>gnu.org</a></p> <p>&emsp;&emsp;虽然调整环境变量可以有很大的收益，这将在下面讨论，简单的建议是将inter_op_parallelism_threads设置为等于物理CPU的数量，并设置以下环境变量。 - KMP_BLOCKTIME=0 - KMP_AFFINITY=granularity=fine,verbose,compact,1,0</p> <p>&emsp;&emsp;有一些模型和硬件平台从不同的设置中受益。下面将讨论影响性能的每个变量。 - KMP_BLOCKTIME。MKL的默认值是200ms，这在我们的测试中并不理想。对于测试的基于CNN的模型，0（0ms）是一个很好的默认值。AlexNex的最佳性能是30ms，GoogleNet和VGG11的最佳性能是1ms。</p> <ul> <li> <p>KMP_AFFINITY：推荐设置为granularity=fine,verbose,compact,1,0。</p> </li> <li> <p>omp_num_threads。这个参数的默认值是物理核心的数量。当使用Intel® Xeon Phi™ (Knights Landing)的某些型号时，调整此参数超过匹配的核心数会产生影响。有关最佳设置，请参阅<a href=https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture>现代英特尔®架构上的TensorFlow*</a>优化。</p> </li> <li> <p>intra_op_parallelism_threads。建议将其设置为等于物理核心的数量。将该值设置为0，这是默认值，将导致该值被设置为逻辑核心的数量，对于某些架构来说是一个可以尝试的选项。这个值和OMP_NUM_THREADS应该相等。</p> </li> <li> <p>inter_op_parallelism_threads: 建议将其设置为等于套接字的数量。将该值设置为0，也就是默认值，会导致该值被设置为逻辑核心的数量。</p> </li> </ul> </article> </div> </div> <a href=# class="md-top md-icon" data-md-component=top data-md-state=hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg> 回到页面顶部 </a> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=页脚> <a href=../pytorch-opt/ class="md-footer__link md-footer__link--prev" aria-label="上一页: PyTorch模型优化思路" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> 上一页 </span> PyTorch模型优化思路 </div> </div> </a> <a href=../latex-manual/ class="md-footer__link md-footer__link--next" aria-label="下一页: Latex环境配置及编译优化" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> 下一页 </span> Latex环境配置及编译优化 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2022 CSWU-Challenge </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <span id=busuanzi_container_site_pv style="color: #276747; font-size: 20px; font-weight: 600;">Total Vister:<span id=busuanzi_value_site_pv></span>Times</span> <div class=md-social> <a href=https://github.com/CSWU-Challeng/CSWU-Challenge.github.io target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.annotate", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.d5a3c699.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../../assets/javascripts/bundle.897f3768.min.js></script> <script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../overrides/assets/javascripts/bundle.a375bc7d.min.js></script> </body> </html>